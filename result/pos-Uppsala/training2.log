2022-08-08 12:55:55,897 ----------------------------------------------------------------------------------------------------
2022-08-08 12:55:55,899 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(42000, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (rnn): LSTM(768, 1024, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=2048, out_features=31, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2022-08-08 12:55:55,899 ----------------------------------------------------------------------------------------------------
2022-08-08 12:55:55,899 Corpus: "Corpus: 24000 train + 3000 dev + 3000 test sentences"
2022-08-08 12:55:55,899 ----------------------------------------------------------------------------------------------------
2022-08-08 12:55:55,899 Parameters:
2022-08-08 12:55:55,899  - learning_rate: "5e-05"
2022-08-08 12:55:55,900  - mini_batch_size: "8"
2022-08-08 12:55:55,900  - patience: "3"
2022-08-08 12:55:55,900  - anneal_factor: "0.5"
2022-08-08 12:55:55,900  - max_epochs: "25"
2022-08-08 12:55:55,900  - shuffle: "True"
2022-08-08 12:55:55,900  - train_with_dev: "False"
2022-08-08 12:55:55,900  - batch_growth_annealing: "False"
2022-08-08 12:55:55,900 ----------------------------------------------------------------------------------------------------
2022-08-08 12:55:55,900 Model training base path: "data/pos-Uppsala/model2"
2022-08-08 12:55:55,900 ----------------------------------------------------------------------------------------------------
2022-08-08 12:55:55,900 Device: cuda:0
2022-08-08 12:55:55,900 ----------------------------------------------------------------------------------------------------
2022-08-08 12:55:55,900 Embeddings storage mode: gpu
2022-08-08 12:55:55,902 ----------------------------------------------------------------------------------------------------
2022-08-08 12:56:44,232 epoch 1 - iter 300/3000 - loss 3.22571085 - samples/sec: 49.68 - lr: 0.000002
2022-08-08 12:57:37,717 epoch 1 - iter 600/3000 - loss 2.77814620 - samples/sec: 44.89 - lr: 0.000004
2022-08-08 12:58:28,577 epoch 1 - iter 900/3000 - loss 2.37006259 - samples/sec: 47.21 - lr: 0.000006
2022-08-08 12:59:24,769 epoch 1 - iter 1200/3000 - loss 1.97124224 - samples/sec: 42.73 - lr: 0.000008
2022-08-08 13:00:20,887 epoch 1 - iter 1500/3000 - loss 1.64437357 - samples/sec: 42.79 - lr: 0.000010
2022-08-08 13:01:14,778 epoch 1 - iter 1800/3000 - loss 1.42680344 - samples/sec: 44.55 - lr: 0.000012
2022-08-08 13:02:08,555 epoch 1 - iter 2100/3000 - loss 1.26757399 - samples/sec: 44.65 - lr: 0.000014
2022-08-08 13:03:04,813 epoch 1 - iter 2400/3000 - loss 1.14233143 - samples/sec: 42.68 - lr: 0.000016
2022-08-08 13:03:58,418 epoch 1 - iter 2700/3000 - loss 1.04505135 - samples/sec: 44.79 - lr: 0.000018
2022-08-08 13:04:51,784 epoch 1 - iter 3000/3000 - loss 0.95552029 - samples/sec: 44.99 - lr: 0.000020
2022-08-08 13:04:51,786 ----------------------------------------------------------------------------------------------------
2022-08-08 13:04:51,786 EPOCH 1 done: loss 0.9555 - lr 0.0000200
2022-08-08 13:09:30,245 DEV : loss 0.11679957062005997 - f1-score (micro avg)  0.9715
2022-08-08 13:09:30,281 BAD EPOCHS (no improvement): 4
2022-08-08 13:09:30,281 ----------------------------------------------------------------------------------------------------
2022-08-08 13:10:25,773 epoch 2 - iter 300/3000 - loss 0.22265115 - samples/sec: 43.28 - lr: 0.000022
2022-08-08 13:11:19,216 epoch 2 - iter 600/3000 - loss 0.20745409 - samples/sec: 44.93 - lr: 0.000024
2022-08-08 13:12:14,422 epoch 2 - iter 900/3000 - loss 0.20100491 - samples/sec: 43.49 - lr: 0.000026
2022-08-08 13:13:09,095 epoch 2 - iter 1200/3000 - loss 0.19499734 - samples/sec: 43.92 - lr: 0.000028
2022-08-08 13:14:04,689 epoch 2 - iter 1500/3000 - loss 0.18837802 - samples/sec: 43.19 - lr: 0.000030
2022-08-08 13:15:02,590 epoch 2 - iter 1800/3000 - loss 0.18466028 - samples/sec: 41.47 - lr: 0.000032
2022-08-08 13:15:57,453 epoch 2 - iter 2100/3000 - loss 0.17980859 - samples/sec: 43.77 - lr: 0.000034
2022-08-08 13:16:52,527 epoch 2 - iter 2400/3000 - loss 0.17450916 - samples/sec: 43.60 - lr: 0.000036
2022-08-08 13:17:47,956 epoch 2 - iter 2700/3000 - loss 0.17042135 - samples/sec: 43.32 - lr: 0.000038
2022-08-08 13:18:43,958 epoch 2 - iter 3000/3000 - loss 0.16713449 - samples/sec: 42.88 - lr: 0.000040
2022-08-08 13:18:43,959 ----------------------------------------------------------------------------------------------------
2022-08-08 13:18:43,960 EPOCH 2 done: loss 0.1671 - lr 0.0000400
2022-08-08 13:23:39,925 DEV : loss 0.10816162824630737 - f1-score (micro avg)  0.9734
2022-08-08 13:23:39,961 BAD EPOCHS (no improvement): 4
2022-08-08 13:23:39,961 ----------------------------------------------------------------------------------------------------
2022-08-08 13:24:35,157 epoch 3 - iter 300/3000 - loss 0.11456096 - samples/sec: 43.50 - lr: 0.000042
2022-08-08 13:25:31,198 epoch 3 - iter 600/3000 - loss 0.10909904 - samples/sec: 42.85 - lr: 0.000044
2022-08-08 13:26:28,739 epoch 3 - iter 900/3000 - loss 0.10720663 - samples/sec: 41.73 - lr: 0.000046
2022-08-08 13:27:24,435 epoch 3 - iter 1200/3000 - loss 0.10614956 - samples/sec: 43.11 - lr: 0.000048
2022-08-08 13:28:21,359 epoch 3 - iter 1500/3000 - loss 0.10641574 - samples/sec: 42.18 - lr: 0.000050
2022-08-08 13:29:19,304 epoch 3 - iter 1800/3000 - loss 0.10593221 - samples/sec: 41.44 - lr: 0.000050
2022-08-08 13:30:14,452 epoch 3 - iter 2100/3000 - loss 0.10574827 - samples/sec: 43.54 - lr: 0.000050
2022-08-08 13:31:10,171 epoch 3 - iter 2400/3000 - loss 0.10573192 - samples/sec: 43.09 - lr: 0.000049
2022-08-08 13:32:06,624 epoch 3 - iter 2700/3000 - loss 0.10565730 - samples/sec: 42.53 - lr: 0.000049
2022-08-08 13:33:03,445 epoch 3 - iter 3000/3000 - loss 0.10464205 - samples/sec: 42.26 - lr: 0.000049
2022-08-08 13:33:03,448 ----------------------------------------------------------------------------------------------------
2022-08-08 13:33:03,448 EPOCH 3 done: loss 0.1046 - lr 0.0000489
2022-08-08 13:37:49,271 DEV : loss 0.10115212202072144 - f1-score (micro avg)  0.9756
2022-08-08 13:37:49,305 BAD EPOCHS (no improvement): 4
2022-08-08 13:37:49,306 ----------------------------------------------------------------------------------------------------
2022-08-08 13:38:43,949 epoch 4 - iter 300/3000 - loss 0.07141276 - samples/sec: 43.94 - lr: 0.000049
2022-08-08 13:39:40,117 epoch 4 - iter 600/3000 - loss 0.07303300 - samples/sec: 42.75 - lr: 0.000048
2022-08-08 13:40:38,027 epoch 4 - iter 900/3000 - loss 0.07282819 - samples/sec: 41.46 - lr: 0.000048
2022-08-08 13:41:34,023 epoch 4 - iter 1200/3000 - loss 0.07365113 - samples/sec: 42.88 - lr: 0.000048
2022-08-08 13:42:31,563 epoch 4 - iter 1500/3000 - loss 0.07479298 - samples/sec: 41.73 - lr: 0.000048
2022-08-08 13:43:32,725 epoch 4 - iter 1800/3000 - loss 0.07620818 - samples/sec: 39.26 - lr: 0.000048
2022-08-08 13:44:30,429 epoch 4 - iter 2100/3000 - loss 0.07624702 - samples/sec: 41.61 - lr: 0.000047
2022-08-08 13:45:26,953 epoch 4 - iter 2400/3000 - loss 0.07686675 - samples/sec: 42.48 - lr: 0.000047
2022-08-08 13:46:21,937 epoch 4 - iter 2700/3000 - loss 0.07698510 - samples/sec: 43.67 - lr: 0.000047
2022-08-08 13:47:17,253 epoch 4 - iter 3000/3000 - loss 0.07639639 - samples/sec: 43.41 - lr: 0.000047
2022-08-08 13:47:17,255 ----------------------------------------------------------------------------------------------------
2022-08-08 13:47:17,255 EPOCH 4 done: loss 0.0764 - lr 0.0000467
2022-08-08 13:52:01,159 DEV : loss 0.11213970184326172 - f1-score (micro avg)  0.9737
2022-08-08 13:52:01,193 BAD EPOCHS (no improvement): 4
2022-08-08 13:52:01,194 ----------------------------------------------------------------------------------------------------
2022-08-08 13:52:57,741 epoch 5 - iter 300/3000 - loss 0.05729797 - samples/sec: 42.47 - lr: 0.000046
2022-08-08 13:53:54,787 epoch 5 - iter 600/3000 - loss 0.05624252 - samples/sec: 42.09 - lr: 0.000046
2022-08-08 13:54:49,725 epoch 5 - iter 900/3000 - loss 0.05658023 - samples/sec: 43.71 - lr: 0.000046
2022-08-08 13:55:45,515 epoch 5 - iter 1200/3000 - loss 0.05689943 - samples/sec: 43.04 - lr: 0.000046
2022-08-08 13:56:44,449 epoch 5 - iter 1500/3000 - loss 0.05737431 - samples/sec: 40.74 - lr: 0.000046
2022-08-08 13:57:39,355 epoch 5 - iter 1800/3000 - loss 0.05798618 - samples/sec: 43.73 - lr: 0.000045
2022-08-08 13:58:38,360 epoch 5 - iter 2100/3000 - loss 0.05811002 - samples/sec: 40.70 - lr: 0.000045
2022-08-08 13:59:34,165 epoch 5 - iter 2400/3000 - loss 0.05787529 - samples/sec: 43.03 - lr: 0.000045
2022-08-08 14:00:30,801 epoch 5 - iter 2700/3000 - loss 0.05803301 - samples/sec: 42.40 - lr: 0.000045
2022-08-08 14:01:27,732 epoch 5 - iter 3000/3000 - loss 0.05790187 - samples/sec: 42.18 - lr: 0.000044
2022-08-08 14:01:27,733 ----------------------------------------------------------------------------------------------------
2022-08-08 14:01:27,733 EPOCH 5 done: loss 0.0579 - lr 0.0000444
2022-08-08 14:06:19,562 DEV : loss 0.11846623569726944 - f1-score (micro avg)  0.9732
2022-08-08 14:06:19,597 BAD EPOCHS (no improvement): 4
2022-08-08 14:06:19,598 ----------------------------------------------------------------------------------------------------
2022-08-08 14:07:15,276 epoch 6 - iter 300/3000 - loss 0.04250618 - samples/sec: 43.13 - lr: 0.000044
2022-08-08 14:08:11,360 epoch 6 - iter 600/3000 - loss 0.04226827 - samples/sec: 42.81 - lr: 0.000044
2022-08-08 14:09:07,644 epoch 6 - iter 900/3000 - loss 0.04202464 - samples/sec: 42.66 - lr: 0.000044
2022-08-08 14:10:03,176 epoch 6 - iter 1200/3000 - loss 0.04331085 - samples/sec: 43.24 - lr: 0.000044
2022-08-08 14:11:02,351 epoch 6 - iter 1500/3000 - loss 0.04317683 - samples/sec: 40.58 - lr: 0.000043
2022-08-08 14:11:57,758 epoch 6 - iter 1800/3000 - loss 0.04321071 - samples/sec: 43.34 - lr: 0.000043
2022-08-08 14:12:54,251 epoch 6 - iter 2100/3000 - loss 0.04373810 - samples/sec: 42.50 - lr: 0.000043
2022-08-08 14:13:50,320 epoch 6 - iter 2400/3000 - loss 0.04384375 - samples/sec: 42.83 - lr: 0.000043
2022-08-08 14:14:47,920 epoch 6 - iter 2700/3000 - loss 0.04436679 - samples/sec: 41.69 - lr: 0.000042
2022-08-08 14:15:43,655 epoch 6 - iter 3000/3000 - loss 0.04530921 - samples/sec: 43.08 - lr: 0.000042
2022-08-08 14:15:43,657 ----------------------------------------------------------------------------------------------------
2022-08-08 14:15:43,657 EPOCH 6 done: loss 0.0453 - lr 0.0000422
2022-08-08 14:20:59,085 DEV : loss 0.1250939816236496 - f1-score (micro avg)  0.9757
2022-08-08 14:20:59,117 BAD EPOCHS (no improvement): 4
2022-08-08 14:20:59,117 ----------------------------------------------------------------------------------------------------
2022-08-08 14:21:54,974 epoch 7 - iter 300/3000 - loss 0.03172761 - samples/sec: 42.99 - lr: 0.000042
2022-08-08 14:22:51,202 epoch 7 - iter 600/3000 - loss 0.03466260 - samples/sec: 42.70 - lr: 0.000042
2022-08-08 14:23:48,379 epoch 7 - iter 900/3000 - loss 0.03658663 - samples/sec: 42.00 - lr: 0.000042
2022-08-08 14:24:46,706 epoch 7 - iter 1200/3000 - loss 0.03630365 - samples/sec: 41.17 - lr: 0.000041
2022-08-08 14:25:46,140 epoch 7 - iter 1500/3000 - loss 0.03597421 - samples/sec: 40.40 - lr: 0.000041
2022-08-08 14:26:43,677 epoch 7 - iter 1800/3000 - loss 0.03659984 - samples/sec: 41.73 - lr: 0.000041
2022-08-08 14:27:39,696 epoch 7 - iter 2100/3000 - loss 0.03642347 - samples/sec: 42.86 - lr: 0.000041
2022-08-08 14:28:36,573 epoch 7 - iter 2400/3000 - loss 0.03656762 - samples/sec: 42.22 - lr: 0.000040
2022-08-08 14:29:33,045 epoch 7 - iter 2700/3000 - loss 0.03678814 - samples/sec: 42.52 - lr: 0.000040
2022-08-08 14:30:29,435 epoch 7 - iter 3000/3000 - loss 0.03647541 - samples/sec: 42.58 - lr: 0.000040
2022-08-08 14:30:29,438 ----------------------------------------------------------------------------------------------------
2022-08-08 14:30:29,438 EPOCH 7 done: loss 0.0365 - lr 0.0000400
2022-08-08 14:35:10,943 DEV : loss 0.14503057301044464 - f1-score (micro avg)  0.974
2022-08-08 14:35:10,977 BAD EPOCHS (no improvement): 4
2022-08-08 14:35:10,977 ----------------------------------------------------------------------------------------------------
2022-08-08 14:36:08,624 epoch 8 - iter 300/3000 - loss 0.02678368 - samples/sec: 41.66 - lr: 0.000040
2022-08-08 14:37:05,495 epoch 8 - iter 600/3000 - loss 0.02718578 - samples/sec: 42.22 - lr: 0.000040
2022-08-08 14:38:01,045 epoch 8 - iter 900/3000 - loss 0.02679784 - samples/sec: 43.22 - lr: 0.000039
2022-08-08 14:38:56,804 epoch 8 - iter 1200/3000 - loss 0.02827196 - samples/sec: 43.06 - lr: 0.000039
2022-08-08 14:39:57,373 epoch 8 - iter 1500/3000 - loss 0.02946317 - samples/sec: 39.64 - lr: 0.000039
2022-08-08 14:40:54,121 epoch 8 - iter 1800/3000 - loss 0.02971080 - samples/sec: 42.31 - lr: 0.000039
2022-08-08 14:41:50,190 epoch 8 - iter 2100/3000 - loss 0.03003842 - samples/sec: 42.83 - lr: 0.000038
2022-08-08 14:42:45,468 epoch 8 - iter 2400/3000 - loss 0.03030482 - samples/sec: 43.44 - lr: 0.000038
2022-08-08 14:43:41,396 epoch 8 - iter 2700/3000 - loss 0.03016835 - samples/sec: 42.93 - lr: 0.000038
2022-08-08 14:44:37,284 epoch 8 - iter 3000/3000 - loss 0.03044406 - samples/sec: 42.96 - lr: 0.000038
2022-08-08 14:44:37,286 ----------------------------------------------------------------------------------------------------
2022-08-08 14:44:37,286 EPOCH 8 done: loss 0.0304 - lr 0.0000378
2022-08-08 14:49:25,528 DEV : loss 0.14661909639835358 - f1-score (micro avg)  0.9756
2022-08-08 14:49:25,564 BAD EPOCHS (no improvement): 4
2022-08-08 14:49:25,565 ----------------------------------------------------------------------------------------------------
2022-08-08 14:50:20,990 epoch 9 - iter 300/3000 - loss 0.02120764 - samples/sec: 43.32 - lr: 0.000038
2022-08-08 14:51:16,744 epoch 9 - iter 600/3000 - loss 0.02107058 - samples/sec: 43.07 - lr: 0.000037
2022-08-08 14:52:13,425 epoch 9 - iter 900/3000 - loss 0.02238180 - samples/sec: 42.36 - lr: 0.000037
2022-08-08 14:53:09,990 epoch 9 - iter 1200/3000 - loss 0.02385716 - samples/sec: 42.45 - lr: 0.000037
2022-08-08 14:54:08,636 epoch 9 - iter 1500/3000 - loss 0.02418016 - samples/sec: 40.94 - lr: 0.000037
2022-08-08 14:55:05,038 epoch 9 - iter 1800/3000 - loss 0.02453822 - samples/sec: 42.57 - lr: 0.000036
2022-08-08 14:56:01,805 epoch 9 - iter 2100/3000 - loss 0.02533484 - samples/sec: 42.30 - lr: 0.000036
2022-08-08 14:56:59,228 epoch 9 - iter 2400/3000 - loss 0.02530711 - samples/sec: 41.82 - lr: 0.000036
2022-08-08 14:57:58,021 epoch 9 - iter 2700/3000 - loss 0.02563063 - samples/sec: 40.84 - lr: 0.000036
2022-08-08 14:58:54,332 epoch 9 - iter 3000/3000 - loss 0.02560246 - samples/sec: 42.64 - lr: 0.000036
2022-08-08 14:58:54,334 ----------------------------------------------------------------------------------------------------
2022-08-08 14:58:54,334 EPOCH 9 done: loss 0.0256 - lr 0.0000356
2022-08-08 15:03:57,103 DEV : loss 0.1587413102388382 - f1-score (micro avg)  0.9749
2022-08-08 15:03:57,138 BAD EPOCHS (no improvement): 4
2022-08-08 15:03:57,138 ----------------------------------------------------------------------------------------------------
2022-08-08 15:04:52,501 epoch 10 - iter 300/3000 - loss 0.02048668 - samples/sec: 43.37 - lr: 0.000035
2022-08-08 15:05:48,608 epoch 10 - iter 600/3000 - loss 0.02065413 - samples/sec: 42.80 - lr: 0.000035
2022-08-08 15:06:44,701 epoch 10 - iter 900/3000 - loss 0.02067199 - samples/sec: 42.81 - lr: 0.000035
2022-08-08 15:07:40,599 epoch 10 - iter 1200/3000 - loss 0.02046527 - samples/sec: 42.96 - lr: 0.000035
2022-08-08 15:08:40,813 epoch 10 - iter 1500/3000 - loss 0.02139262 - samples/sec: 39.88 - lr: 0.000034
2022-08-08 15:09:36,769 epoch 10 - iter 1800/3000 - loss 0.02144042 - samples/sec: 42.91 - lr: 0.000034
2022-08-08 15:10:33,419 epoch 10 - iter 2100/3000 - loss 0.02172268 - samples/sec: 42.39 - lr: 0.000034
2022-08-08 15:11:31,778 epoch 10 - iter 2400/3000 - loss 0.02188812 - samples/sec: 41.15 - lr: 0.000034
2022-08-08 15:12:28,024 epoch 10 - iter 2700/3000 - loss 0.02189028 - samples/sec: 42.69 - lr: 0.000034
2022-08-08 15:13:24,299 epoch 10 - iter 3000/3000 - loss 0.02180444 - samples/sec: 42.67 - lr: 0.000033
2022-08-08 15:13:24,301 ----------------------------------------------------------------------------------------------------
2022-08-08 15:13:24,301 EPOCH 10 done: loss 0.0218 - lr 0.0000333
2022-08-08 15:18:24,603 DEV : loss 0.18945468962192535 - f1-score (micro avg)  0.9757
2022-08-08 15:18:24,638 BAD EPOCHS (no improvement): 4
2022-08-08 15:18:24,639 ----------------------------------------------------------------------------------------------------
2022-08-08 15:19:20,571 epoch 11 - iter 300/3000 - loss 0.01808809 - samples/sec: 42.93 - lr: 0.000033
2022-08-08 15:20:16,379 epoch 11 - iter 600/3000 - loss 0.01814668 - samples/sec: 43.03 - lr: 0.000033
2022-08-08 15:21:13,691 epoch 11 - iter 900/3000 - loss 0.01877936 - samples/sec: 41.90 - lr: 0.000033
2022-08-08 15:22:10,397 epoch 11 - iter 1200/3000 - loss 0.01868408 - samples/sec: 42.35 - lr: 0.000032
2022-08-08 15:23:11,435 epoch 11 - iter 1500/3000 - loss 0.01818086 - samples/sec: 39.34 - lr: 0.000032
2022-08-08 15:24:07,293 epoch 11 - iter 1800/3000 - loss 0.01788887 - samples/sec: 42.99 - lr: 0.000032
2022-08-08 15:25:04,481 epoch 11 - iter 2100/3000 - loss 0.01865563 - samples/sec: 41.99 - lr: 0.000032
2022-08-08 15:26:00,487 epoch 11 - iter 2400/3000 - loss 0.01862052 - samples/sec: 42.87 - lr: 0.000032
2022-08-08 15:26:57,405 epoch 11 - iter 2700/3000 - loss 0.01887445 - samples/sec: 42.19 - lr: 0.000031
2022-08-08 15:27:54,938 epoch 11 - iter 3000/3000 - loss 0.01922704 - samples/sec: 41.74 - lr: 0.000031
2022-08-08 15:27:54,940 ----------------------------------------------------------------------------------------------------
2022-08-08 15:27:54,940 EPOCH 11 done: loss 0.0192 - lr 0.0000311
2022-08-08 15:32:54,559 DEV : loss 0.2052067667245865 - f1-score (micro avg)  0.9751
2022-08-08 15:32:54,595 BAD EPOCHS (no improvement): 4
2022-08-08 15:32:54,596 ----------------------------------------------------------------------------------------------------
2022-08-08 15:33:51,665 epoch 12 - iter 300/3000 - loss 0.01629973 - samples/sec: 42.08 - lr: 0.000031
2022-08-08 15:34:49,390 epoch 12 - iter 600/3000 - loss 0.01810201 - samples/sec: 41.60 - lr: 0.000031
2022-08-08 15:35:46,172 epoch 12 - iter 900/3000 - loss 0.01695171 - samples/sec: 42.29 - lr: 0.000030
2022-08-08 15:36:46,549 epoch 12 - iter 1200/3000 - loss 0.01650331 - samples/sec: 39.77 - lr: 0.000030
2022-08-08 15:37:42,015 epoch 12 - iter 1500/3000 - loss 0.01656925 - samples/sec: 43.29 - lr: 0.000030
2022-08-08 15:38:38,520 epoch 12 - iter 1800/3000 - loss 0.01639623 - samples/sec: 42.50 - lr: 0.000030
2022-08-08 15:39:36,163 epoch 12 - iter 2100/3000 - loss 0.01636147 - samples/sec: 41.66 - lr: 0.000030
2022-08-08 15:40:35,161 epoch 12 - iter 2400/3000 - loss 0.01647329 - samples/sec: 40.70 - lr: 0.000029
2022-08-08 15:41:31,688 epoch 12 - iter 2700/3000 - loss 0.01623124 - samples/sec: 42.48 - lr: 0.000029
2022-08-08 15:42:28,276 epoch 12 - iter 3000/3000 - loss 0.01630833 - samples/sec: 42.43 - lr: 0.000029
2022-08-08 15:42:28,278 ----------------------------------------------------------------------------------------------------
2022-08-08 15:42:28,278 EPOCH 12 done: loss 0.0163 - lr 0.0000289
2022-08-08 15:47:15,076 DEV : loss 0.2101939618587494 - f1-score (micro avg)  0.9762
2022-08-08 15:47:15,111 BAD EPOCHS (no improvement): 4
2022-08-08 15:47:15,111 ----------------------------------------------------------------------------------------------------
2022-08-08 15:48:12,519 epoch 13 - iter 300/3000 - loss 0.01409821 - samples/sec: 41.83 - lr: 0.000029
2022-08-08 15:49:08,826 epoch 13 - iter 600/3000 - loss 0.01398811 - samples/sec: 42.65 - lr: 0.000028
2022-08-08 15:50:06,884 epoch 13 - iter 900/3000 - loss 0.01516546 - samples/sec: 41.36 - lr: 0.000028
2022-08-08 15:51:07,103 epoch 13 - iter 1200/3000 - loss 0.01477019 - samples/sec: 39.87 - lr: 0.000028
2022-08-08 15:52:03,446 epoch 13 - iter 1500/3000 - loss 0.01439904 - samples/sec: 42.62 - lr: 0.000028
2022-08-08 15:52:58,676 epoch 13 - iter 1800/3000 - loss 0.01424516 - samples/sec: 43.48 - lr: 0.000028
2022-08-08 15:53:54,934 epoch 13 - iter 2100/3000 - loss 0.01382919 - samples/sec: 42.68 - lr: 0.000027
2022-08-08 15:54:51,039 epoch 13 - iter 2400/3000 - loss 0.01394308 - samples/sec: 42.80 - lr: 0.000027
2022-08-08 15:55:47,491 epoch 13 - iter 2700/3000 - loss 0.01431148 - samples/sec: 42.54 - lr: 0.000027
2022-08-08 15:56:50,957 epoch 13 - iter 3000/3000 - loss 0.01404566 - samples/sec: 37.83 - lr: 0.000027
2022-08-08 15:56:50,959 ----------------------------------------------------------------------------------------------------
2022-08-08 15:56:50,959 EPOCH 13 done: loss 0.0140 - lr 0.0000267
2022-08-08 16:01:46,268 DEV : loss 0.24013161659240723 - f1-score (micro avg)  0.9754
2022-08-08 16:01:46,304 BAD EPOCHS (no improvement): 4
2022-08-08 16:01:46,304 ----------------------------------------------------------------------------------------------------
2022-08-08 16:02:44,411 epoch 14 - iter 300/3000 - loss 0.01509174 - samples/sec: 41.33 - lr: 0.000026
2022-08-08 16:03:41,209 epoch 14 - iter 600/3000 - loss 0.01359098 - samples/sec: 42.28 - lr: 0.000026
2022-08-08 16:04:38,982 epoch 14 - iter 900/3000 - loss 0.01293668 - samples/sec: 41.57 - lr: 0.000026
2022-08-08 16:05:38,769 epoch 14 - iter 1200/3000 - loss 0.01360557 - samples/sec: 40.16 - lr: 0.000026
2022-08-08 16:06:35,300 epoch 14 - iter 1500/3000 - loss 0.01282610 - samples/sec: 42.48 - lr: 0.000026
2022-08-08 16:07:32,130 epoch 14 - iter 1800/3000 - loss 0.01260112 - samples/sec: 42.25 - lr: 0.000025
2022-08-08 16:08:28,153 epoch 14 - iter 2100/3000 - loss 0.01256282 - samples/sec: 42.86 - lr: 0.000025
2022-08-08 16:09:25,023 epoch 14 - iter 2400/3000 - loss 0.01239687 - samples/sec: 42.22 - lr: 0.000025
2022-08-08 16:10:21,860 epoch 14 - iter 2700/3000 - loss 0.01258998 - samples/sec: 42.25 - lr: 0.000025
2022-08-08 16:11:21,419 epoch 14 - iter 3000/3000 - loss 0.01294029 - samples/sec: 40.32 - lr: 0.000024
2022-08-08 16:11:21,421 ----------------------------------------------------------------------------------------------------
2022-08-08 16:11:21,421 EPOCH 14 done: loss 0.0129 - lr 0.0000245
2022-08-08 16:16:18,790 DEV : loss 0.25191810727119446 - f1-score (micro avg)  0.9759
2022-08-08 16:16:18,826 BAD EPOCHS (no improvement): 4
2022-08-08 16:16:18,827 ----------------------------------------------------------------------------------------------------
2022-08-08 16:17:14,863 epoch 15 - iter 300/3000 - loss 0.01139393 - samples/sec: 42.85 - lr: 0.000024
2022-08-08 16:18:11,342 epoch 15 - iter 600/3000 - loss 0.01111429 - samples/sec: 42.52 - lr: 0.000024
2022-08-08 16:19:11,106 epoch 15 - iter 900/3000 - loss 0.01099871 - samples/sec: 40.18 - lr: 0.000024
2022-08-08 16:20:07,587 epoch 15 - iter 1200/3000 - loss 0.01068302 - samples/sec: 42.52 - lr: 0.000024
2022-08-08 16:21:05,221 epoch 15 - iter 1500/3000 - loss 0.01040129 - samples/sec: 41.66 - lr: 0.000023
2022-08-08 16:22:01,876 epoch 15 - iter 1800/3000 - loss 0.01053939 - samples/sec: 42.38 - lr: 0.000023
2022-08-08 16:22:57,725 epoch 15 - iter 2100/3000 - loss 0.01110258 - samples/sec: 43.00 - lr: 0.000023
2022-08-08 16:23:54,740 epoch 15 - iter 2400/3000 - loss 0.01107704 - samples/sec: 42.12 - lr: 0.000023
2022-08-08 16:24:55,711 epoch 15 - iter 2700/3000 - loss 0.01104789 - samples/sec: 39.38 - lr: 0.000022
2022-08-08 16:25:51,882 epoch 15 - iter 3000/3000 - loss 0.01125168 - samples/sec: 42.75 - lr: 0.000022
2022-08-08 16:25:51,884 ----------------------------------------------------------------------------------------------------
2022-08-08 16:25:51,884 EPOCH 15 done: loss 0.0113 - lr 0.0000222
2022-08-08 16:30:41,818 DEV : loss 0.2727336883544922 - f1-score (micro avg)  0.9761
2022-08-08 16:30:41,853 BAD EPOCHS (no improvement): 4
2022-08-08 16:30:41,854 ----------------------------------------------------------------------------------------------------
2022-08-08 16:31:37,786 epoch 16 - iter 300/3000 - loss 0.00795406 - samples/sec: 42.93 - lr: 0.000022
2022-08-08 16:32:36,379 epoch 16 - iter 600/3000 - loss 0.00776572 - samples/sec: 40.99 - lr: 0.000022
2022-08-08 16:33:38,551 epoch 16 - iter 900/3000 - loss 0.00834192 - samples/sec: 38.62 - lr: 0.000022
2022-08-08 16:34:36,461 epoch 16 - iter 1200/3000 - loss 0.00788826 - samples/sec: 41.47 - lr: 0.000021
2022-08-08 16:35:32,844 epoch 16 - iter 1500/3000 - loss 0.00808803 - samples/sec: 42.59 - lr: 0.000021
2022-08-08 16:36:29,402 epoch 16 - iter 1800/3000 - loss 0.00867750 - samples/sec: 42.46 - lr: 0.000021
2022-08-08 16:37:25,957 epoch 16 - iter 2100/3000 - loss 0.00858325 - samples/sec: 42.46 - lr: 0.000021
2022-08-08 16:38:22,422 epoch 16 - iter 2400/3000 - loss 0.00832360 - samples/sec: 42.53 - lr: 0.000020
2022-08-08 16:39:24,044 epoch 16 - iter 2700/3000 - loss 0.00842312 - samples/sec: 38.97 - lr: 0.000020
2022-08-08 16:40:21,210 epoch 16 - iter 3000/3000 - loss 0.00852568 - samples/sec: 42.01 - lr: 0.000020
2022-08-08 16:40:21,212 ----------------------------------------------------------------------------------------------------
2022-08-08 16:40:21,212 EPOCH 16 done: loss 0.0085 - lr 0.0000200
2022-08-08 16:45:05,655 DEV : loss 0.28033459186553955 - f1-score (micro avg)  0.976
2022-08-08 16:45:05,691 BAD EPOCHS (no improvement): 4
2022-08-08 16:45:05,691 ----------------------------------------------------------------------------------------------------
2022-08-08 16:46:01,914 epoch 17 - iter 300/3000 - loss 0.00623170 - samples/sec: 42.71 - lr: 0.000020
2022-08-08 16:46:57,540 epoch 17 - iter 600/3000 - loss 0.00579569 - samples/sec: 43.17 - lr: 0.000020
2022-08-08 16:47:57,388 epoch 17 - iter 900/3000 - loss 0.00731326 - samples/sec: 40.12 - lr: 0.000019
2022-08-08 16:48:53,702 epoch 17 - iter 1200/3000 - loss 0.00771105 - samples/sec: 42.64 - lr: 0.000019
2022-08-08 16:49:50,908 epoch 17 - iter 1500/3000 - loss 0.00755834 - samples/sec: 41.98 - lr: 0.000019
2022-08-08 16:50:47,699 epoch 17 - iter 1800/3000 - loss 0.00750874 - samples/sec: 42.28 - lr: 0.000019
2022-08-08 16:51:43,453 epoch 17 - iter 2100/3000 - loss 0.00724732 - samples/sec: 43.07 - lr: 0.000018
2022-08-08 16:52:40,950 epoch 17 - iter 2400/3000 - loss 0.00723705 - samples/sec: 41.76 - lr: 0.000018
2022-08-08 16:53:42,381 epoch 17 - iter 2700/3000 - loss 0.00740812 - samples/sec: 39.09 - lr: 0.000018
2022-08-08 16:54:39,328 epoch 17 - iter 3000/3000 - loss 0.00761184 - samples/sec: 42.17 - lr: 0.000018
2022-08-08 16:54:39,330 ----------------------------------------------------------------------------------------------------
2022-08-08 16:54:39,330 EPOCH 17 done: loss 0.0076 - lr 0.0000178
2022-08-08 16:59:35,966 DEV : loss 0.29163315892219543 - f1-score (micro avg)  0.9763
2022-08-08 16:59:36,001 BAD EPOCHS (no improvement): 4
2022-08-08 16:59:36,001 ----------------------------------------------------------------------------------------------------
2022-08-08 17:00:33,719 epoch 18 - iter 300/3000 - loss 0.00490462 - samples/sec: 41.61 - lr: 0.000018
2022-08-08 17:01:30,676 epoch 18 - iter 600/3000 - loss 0.00480305 - samples/sec: 42.16 - lr: 0.000017
2022-08-08 17:02:33,135 epoch 18 - iter 900/3000 - loss 0.00557512 - samples/sec: 38.44 - lr: 0.000017
2022-08-08 17:03:30,196 epoch 18 - iter 1200/3000 - loss 0.00541081 - samples/sec: 42.08 - lr: 0.000017
2022-08-08 17:04:26,718 epoch 18 - iter 1500/3000 - loss 0.00615035 - samples/sec: 42.48 - lr: 0.000017
2022-08-08 17:05:24,355 epoch 18 - iter 1800/3000 - loss 0.00608157 - samples/sec: 41.66 - lr: 0.000016
2022-08-08 17:06:21,260 epoch 18 - iter 2100/3000 - loss 0.00589186 - samples/sec: 42.20 - lr: 0.000016
2022-08-08 17:07:17,373 epoch 18 - iter 2400/3000 - loss 0.00609054 - samples/sec: 42.79 - lr: 0.000016
2022-08-08 17:08:16,684 epoch 18 - iter 2700/3000 - loss 0.00593379 - samples/sec: 40.49 - lr: 0.000016
2022-08-08 17:09:14,562 epoch 18 - iter 3000/3000 - loss 0.00613705 - samples/sec: 41.49 - lr: 0.000016
2022-08-08 17:09:14,564 ----------------------------------------------------------------------------------------------------
2022-08-08 17:09:14,564 EPOCH 18 done: loss 0.0061 - lr 0.0000156
2022-08-08 17:14:04,864 DEV : loss 0.31640198826789856 - f1-score (micro avg)  0.976
2022-08-08 17:14:04,897 BAD EPOCHS (no improvement): 4
2022-08-08 17:14:04,897 ----------------------------------------------------------------------------------------------------
2022-08-08 17:15:00,683 epoch 19 - iter 300/3000 - loss 0.00635701 - samples/sec: 43.05 - lr: 0.000015
2022-08-08 17:15:54,683 epoch 19 - iter 600/3000 - loss 0.00557888 - samples/sec: 44.47 - lr: 0.000015
2022-08-08 17:16:53,385 epoch 19 - iter 900/3000 - loss 0.00539637 - samples/sec: 40.91 - lr: 0.000015
2022-08-08 17:17:49,375 epoch 19 - iter 1200/3000 - loss 0.00509621 - samples/sec: 42.89 - lr: 0.000015
2022-08-08 17:18:45,885 epoch 19 - iter 1500/3000 - loss 0.00506106 - samples/sec: 42.49 - lr: 0.000014
2022-08-08 17:19:43,234 epoch 19 - iter 1800/3000 - loss 0.00499363 - samples/sec: 41.87 - lr: 0.000014
2022-08-08 17:20:39,768 epoch 19 - iter 2100/3000 - loss 0.00527957 - samples/sec: 42.47 - lr: 0.000014
2022-08-08 17:21:37,306 epoch 19 - iter 2400/3000 - loss 0.00544344 - samples/sec: 41.74 - lr: 0.000014
2022-08-08 17:22:37,748 epoch 19 - iter 2700/3000 - loss 0.00528327 - samples/sec: 39.73 - lr: 0.000014
2022-08-08 17:23:33,730 epoch 19 - iter 3000/3000 - loss 0.00513966 - samples/sec: 42.89 - lr: 0.000013
2022-08-08 17:23:33,732 ----------------------------------------------------------------------------------------------------
2022-08-08 17:23:33,732 EPOCH 19 done: loss 0.0051 - lr 0.0000133
2022-08-08 17:28:38,694 DEV : loss 0.3111695647239685 - f1-score (micro avg)  0.9763
2022-08-08 17:28:38,733 BAD EPOCHS (no improvement): 4
2022-08-08 17:28:38,733 ----------------------------------------------------------------------------------------------------
2022-08-08 17:29:37,498 epoch 20 - iter 300/3000 - loss 0.00381864 - samples/sec: 40.87 - lr: 0.000013
2022-08-08 17:30:37,628 epoch 20 - iter 600/3000 - loss 0.00421252 - samples/sec: 39.93 - lr: 0.000013
2022-08-08 17:31:33,423 epoch 20 - iter 900/3000 - loss 0.00493560 - samples/sec: 43.04 - lr: 0.000013
2022-08-08 17:32:30,332 epoch 20 - iter 1200/3000 - loss 0.00457834 - samples/sec: 42.20 - lr: 0.000012
2022-08-08 17:33:27,737 epoch 20 - iter 1500/3000 - loss 0.00453591 - samples/sec: 41.83 - lr: 0.000012
2022-08-08 17:34:24,979 epoch 20 - iter 1800/3000 - loss 0.00440653 - samples/sec: 41.95 - lr: 0.000012
2022-08-08 17:35:22,644 epoch 20 - iter 2100/3000 - loss 0.00446984 - samples/sec: 41.64 - lr: 0.000012
2022-08-08 17:36:21,513 epoch 20 - iter 2400/3000 - loss 0.00448916 - samples/sec: 40.79 - lr: 0.000012
2022-08-08 17:37:19,387 epoch 20 - iter 2700/3000 - loss 0.00463438 - samples/sec: 41.49 - lr: 0.000011
2022-08-08 17:38:15,599 epoch 20 - iter 3000/3000 - loss 0.00463788 - samples/sec: 42.72 - lr: 0.000011
2022-08-08 17:38:15,601 ----------------------------------------------------------------------------------------------------
2022-08-08 17:38:15,601 EPOCH 20 done: loss 0.0046 - lr 0.0000111
2022-08-08 17:43:14,075 DEV : loss 0.3210408687591553 - f1-score (micro avg)  0.9766
2022-08-08 17:43:14,106 BAD EPOCHS (no improvement): 4
2022-08-08 17:43:14,107 ----------------------------------------------------------------------------------------------------
2022-08-08 17:44:10,294 epoch 21 - iter 300/3000 - loss 0.00334110 - samples/sec: 42.74 - lr: 0.000011
2022-08-08 17:45:09,595 epoch 21 - iter 600/3000 - loss 0.00383793 - samples/sec: 40.49 - lr: 0.000011
2022-08-08 17:46:06,431 epoch 21 - iter 900/3000 - loss 0.00378089 - samples/sec: 42.25 - lr: 0.000010
2022-08-08 17:47:03,135 epoch 21 - iter 1200/3000 - loss 0.00355312 - samples/sec: 42.35 - lr: 0.000010
2022-08-08 17:48:02,276 epoch 21 - iter 1500/3000 - loss 0.00331217 - samples/sec: 40.60 - lr: 0.000010
2022-08-08 17:48:58,645 epoch 21 - iter 1800/3000 - loss 0.00330348 - samples/sec: 42.60 - lr: 0.000010
2022-08-08 17:49:55,992 epoch 21 - iter 2100/3000 - loss 0.00323618 - samples/sec: 41.87 - lr: 0.000010
2022-08-08 17:50:56,257 epoch 21 - iter 2400/3000 - loss 0.00331135 - samples/sec: 39.84 - lr: 0.000009
2022-08-08 17:51:51,950 epoch 21 - iter 2700/3000 - loss 0.00331446 - samples/sec: 43.12 - lr: 0.000009
2022-08-08 17:52:48,312 epoch 21 - iter 3000/3000 - loss 0.00326231 - samples/sec: 42.60 - lr: 0.000009
2022-08-08 17:52:48,314 ----------------------------------------------------------------------------------------------------
2022-08-08 17:52:48,314 EPOCH 21 done: loss 0.0033 - lr 0.0000089
2022-08-08 17:57:44,376 DEV : loss 0.32771098613739014 - f1-score (micro avg)  0.9769
2022-08-08 17:57:44,413 BAD EPOCHS (no improvement): 4
2022-08-08 17:57:44,413 ----------------------------------------------------------------------------------------------------
2022-08-08 17:58:42,808 epoch 22 - iter 300/3000 - loss 0.00254603 - samples/sec: 41.12 - lr: 0.000009
2022-08-08 17:59:43,209 epoch 22 - iter 600/3000 - loss 0.00270058 - samples/sec: 39.75 - lr: 0.000008
2022-08-08 18:00:38,839 epoch 22 - iter 900/3000 - loss 0.00251241 - samples/sec: 43.16 - lr: 0.000008
2022-08-08 18:01:35,647 epoch 22 - iter 1200/3000 - loss 0.00288530 - samples/sec: 42.27 - lr: 0.000008
2022-08-08 18:02:31,083 epoch 22 - iter 1500/3000 - loss 0.00271892 - samples/sec: 43.32 - lr: 0.000008
2022-08-08 18:03:28,270 epoch 22 - iter 1800/3000 - loss 0.00258326 - samples/sec: 41.99 - lr: 0.000008
2022-08-08 18:04:26,974 epoch 22 - iter 2100/3000 - loss 0.00281009 - samples/sec: 40.91 - lr: 0.000007
2022-08-08 18:05:27,043 epoch 22 - iter 2400/3000 - loss 0.00278963 - samples/sec: 39.97 - lr: 0.000007
2022-08-08 18:06:23,293 epoch 22 - iter 2700/3000 - loss 0.00300171 - samples/sec: 42.69 - lr: 0.000007
2022-08-08 18:07:19,407 epoch 22 - iter 3000/3000 - loss 0.00294315 - samples/sec: 42.79 - lr: 0.000007
2022-08-08 18:07:19,409 ----------------------------------------------------------------------------------------------------
2022-08-08 18:07:19,409 EPOCH 22 done: loss 0.0029 - lr 0.0000067
2022-08-08 18:12:16,928 DEV : loss 0.3352717161178589 - f1-score (micro avg)  0.9767
2022-08-08 18:12:16,964 BAD EPOCHS (no improvement): 4
2022-08-08 18:12:16,964 ----------------------------------------------------------------------------------------------------
2022-08-08 18:13:16,531 epoch 23 - iter 300/3000 - loss 0.00217983 - samples/sec: 40.31 - lr: 0.000006
2022-08-08 18:14:13,385 epoch 23 - iter 600/3000 - loss 0.00224429 - samples/sec: 42.24 - lr: 0.000006
2022-08-08 18:15:08,901 epoch 23 - iter 900/3000 - loss 0.00233429 - samples/sec: 43.25 - lr: 0.000006
2022-08-08 18:16:05,557 epoch 23 - iter 1200/3000 - loss 0.00235094 - samples/sec: 42.38 - lr: 0.000006
2022-08-08 18:17:02,478 epoch 23 - iter 1500/3000 - loss 0.00232250 - samples/sec: 42.19 - lr: 0.000006
2022-08-08 18:17:59,782 epoch 23 - iter 1800/3000 - loss 0.00239107 - samples/sec: 41.90 - lr: 0.000005
2022-08-08 18:18:56,554 epoch 23 - iter 2100/3000 - loss 0.00226168 - samples/sec: 42.30 - lr: 0.000005
2022-08-08 18:19:55,343 epoch 23 - iter 2400/3000 - loss 0.00224220 - samples/sec: 40.84 - lr: 0.000005
2022-08-08 18:20:52,143 epoch 23 - iter 2700/3000 - loss 0.00230182 - samples/sec: 42.28 - lr: 0.000005
2022-08-08 18:21:48,920 epoch 23 - iter 3000/3000 - loss 0.00230620 - samples/sec: 42.29 - lr: 0.000004
2022-08-08 18:21:48,922 ----------------------------------------------------------------------------------------------------
2022-08-08 18:21:48,922 EPOCH 23 done: loss 0.0023 - lr 0.0000045
2022-08-08 18:26:45,546 DEV : loss 0.33948418498039246 - f1-score (micro avg)  0.9767
2022-08-08 18:26:45,581 BAD EPOCHS (no improvement): 4
2022-08-08 18:26:45,582 ----------------------------------------------------------------------------------------------------
2022-08-08 18:27:45,246 epoch 24 - iter 300/3000 - loss 0.00219641 - samples/sec: 40.25 - lr: 0.000004
2022-08-08 18:28:41,834 epoch 24 - iter 600/3000 - loss 0.00245748 - samples/sec: 42.43 - lr: 0.000004
2022-08-08 18:29:39,077 epoch 24 - iter 900/3000 - loss 0.00250871 - samples/sec: 41.95 - lr: 0.000004
2022-08-08 18:30:34,731 epoch 24 - iter 1200/3000 - loss 0.00205753 - samples/sec: 43.15 - lr: 0.000004
2022-08-08 18:31:30,571 epoch 24 - iter 1500/3000 - loss 0.00197538 - samples/sec: 43.00 - lr: 0.000003
2022-08-08 18:32:28,737 epoch 24 - iter 1800/3000 - loss 0.00196254 - samples/sec: 41.28 - lr: 0.000003
2022-08-08 18:33:29,253 epoch 24 - iter 2100/3000 - loss 0.00197466 - samples/sec: 39.68 - lr: 0.000003
2022-08-08 18:34:25,737 epoch 24 - iter 2400/3000 - loss 0.00206992 - samples/sec: 42.51 - lr: 0.000003
2022-08-08 18:35:23,065 epoch 24 - iter 2700/3000 - loss 0.00206708 - samples/sec: 41.89 - lr: 0.000002
2022-08-08 18:36:20,803 epoch 24 - iter 3000/3000 - loss 0.00217669 - samples/sec: 41.59 - lr: 0.000002
2022-08-08 18:36:20,805 ----------------------------------------------------------------------------------------------------
2022-08-08 18:36:20,805 EPOCH 24 done: loss 0.0022 - lr 0.0000022
2022-08-08 18:41:20,591 DEV : loss 0.3365582823753357 - f1-score (micro avg)  0.9771
2022-08-08 18:41:20,626 BAD EPOCHS (no improvement): 4
2022-08-08 18:41:20,626 ----------------------------------------------------------------------------------------------------
2022-08-08 18:42:19,475 epoch 25 - iter 300/3000 - loss 0.00214913 - samples/sec: 40.80 - lr: 0.000002
2022-08-08 18:43:15,070 epoch 25 - iter 600/3000 - loss 0.00191199 - samples/sec: 43.19 - lr: 0.000002
2022-08-08 18:44:12,697 epoch 25 - iter 900/3000 - loss 0.00199682 - samples/sec: 41.67 - lr: 0.000002
2022-08-08 18:45:10,576 epoch 25 - iter 1200/3000 - loss 0.00198456 - samples/sec: 41.49 - lr: 0.000001
2022-08-08 18:46:06,509 epoch 25 - iter 1500/3000 - loss 0.00185792 - samples/sec: 42.93 - lr: 0.000001
2022-08-08 18:47:02,751 epoch 25 - iter 1800/3000 - loss 0.00203262 - samples/sec: 42.69 - lr: 0.000001
2022-08-08 18:48:03,592 epoch 25 - iter 2100/3000 - loss 0.00202945 - samples/sec: 39.47 - lr: 0.000001
2022-08-08 18:49:01,245 epoch 25 - iter 2400/3000 - loss 0.00192699 - samples/sec: 41.65 - lr: 0.000000
2022-08-08 18:49:58,158 epoch 25 - iter 2700/3000 - loss 0.00187291 - samples/sec: 42.19 - lr: 0.000000
2022-08-08 18:50:55,025 epoch 25 - iter 3000/3000 - loss 0.00185813 - samples/sec: 42.23 - lr: 0.000000
2022-08-08 18:50:55,026 ----------------------------------------------------------------------------------------------------
2022-08-08 18:50:55,027 EPOCH 25 done: loss 0.0019 - lr 0.0000000
2022-08-08 18:55:46,583 DEV : loss 0.3365671634674072 - f1-score (micro avg)  0.9771
2022-08-08 18:55:46,618 BAD EPOCHS (no improvement): 4
2022-08-08 18:55:47,735 ----------------------------------------------------------------------------------------------------
2022-08-08 18:55:47,736 Testing using last state of model ...
2022-08-08 18:59:36,778 0.9665	0.9665	0.9665	0.9665
2022-08-08 18:59:36,779 
Results:
- F-score (micro) 0.9665
- F-score (macro) 0.866
- Accuracy 0.9665

By class:
              precision    recall  f1-score   support

      N_SING     0.9732    0.9620    0.9675     30553
           P     0.9566    0.9939    0.9749      9951
        DELM     0.9953    0.9873    0.9913      8122
         ADJ     0.9199    0.9356    0.9277      7466
         CON     0.9924    0.9798    0.9861      6823
        N_PL     0.9775    0.9611    0.9692      5163
        V_PA     0.9797    0.9739    0.9768      2873
       V_PRS     0.9839    0.9926    0.9883      2841
         PRO     0.9623    0.9504    0.9563      2258
         NUM     0.9902    0.9973    0.9937      2232
         DET     0.9539    0.9611    0.9575      1853
      CLITIC     1.0000    1.0000    1.0000      1259
        V_PP     0.9740    0.9698    0.9719      1158
       V_SUB     0.9658    0.9593    0.9625      1031
         ADV     0.7667    0.8068    0.7863       880
    ADV_TIME     0.9027    0.9489    0.9252       489
       V_AUX     0.9793    0.9974    0.9882       379
     ADJ_SUP     0.9741    0.9741    0.9741       270
    ADJ_CMPR     0.8852    0.9585    0.9204       193
     ADJ_INO     0.7770    0.6845    0.7278       168
     ADV_NEG     0.9065    0.8456    0.8750       149
       ADV_I     0.8696    0.8571    0.8633       140
          FW     0.6609    0.6179    0.6387       123
    ADV_COMP     0.8000    0.9474    0.8675        76
     ADV_LOC     0.9589    0.9589    0.9589        73
       V_IMP     0.6667    0.7143    0.6897        56
        PREV     0.8667    0.8125    0.8387        32
         INT     0.3871    0.5000    0.4364        24
       N_VOC     0.0000    0.0000    0.0000         0

   micro avg     0.9665    0.9665    0.9665     86635
   macro avg     0.8630    0.8706    0.8660     86635
weighted avg     0.9668    0.9665    0.9666     86635
 samples avg     0.9665    0.9665    0.9665     86635

2022-08-08 18:59:36,779 ----------------------------------------------------------------------------------------------------
